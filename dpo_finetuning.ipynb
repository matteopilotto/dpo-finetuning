{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc7ea7ab-cfaf-47df-b076-aceb43cf5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec32cda-45da-43af-b5dc-025f185a596f",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4520f3a-ac5b-44c3-a4e2-68ff948beb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0634326c324c018108ddbc04fbcdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    # split=\"train\",\n",
    "    # data_dir=\"data/rl\"\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c102f7f-8766-4b94-9ccb-93eb0c97a5f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "        n_shards: 72\n",
       "    })\n",
       "    test: IterableDataset({\n",
       "        features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "        n_shards: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53839fda-c7ac-4799-aa25-fdc42eef3d82",
   "metadata": {},
   "source": [
    "### Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ae6318a-6dfe-4717-8a54-5feb9b42f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing 1000 of 10000...\n",
      "[INFO] processing 2000 of 10000...\n",
      "[INFO] processing 3000 of 10000...\n",
      "[INFO] processing 4000 of 10000...\n",
      "[INFO] processing 5000 of 10000...\n",
      "[INFO] processing 6000 of 10000...\n",
      "[INFO] processing 7000 of 10000...\n",
      "[INFO] processing 8000 of 10000...\n",
      "[INFO] processing 9000 of 10000...\n",
      "[INFO] processing 10000 of 10000...\n",
      "[INFO] processing 1000 of 10000...\n",
      "[INFO] processing 2000 of 10000...\n",
      "[INFO] processing 3000 of 10000...\n",
      "[INFO] processing 4000 of 10000...\n",
      "[INFO] processing 5000 of 10000...\n",
      "[INFO] processing 6000 of 10000...\n",
      "[INFO] processing 7000 of 10000...\n",
      "[INFO] processing 8000 of 10000...\n",
      "[INFO] processing 9000 of 10000...\n",
      "[INFO] processing 10000 of 10000...\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "train_sample_data = []\n",
    "test_sample_data = []\n",
    "\n",
    "for i, example in enumerate(train_ds):\n",
    "    if i == sample_size:\n",
    "        break\n",
    "\n",
    "    train_sample_data.append(example)\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"[INFO] processing {i+1} of {sample_size}...\")\n",
    "\n",
    "for i, example in enumerate(test_ds):\n",
    "    if i == sample_size:\n",
    "        break\n",
    "\n",
    "    test_sample_data.append(example)\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"[INFO] processing {i+1} of {sample_size}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf477efc-2916-43e2-a443-087b0f3b115f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds_sample = Dataset.from_list(train_sample_data)\n",
    "test_ds_sample = Dataset.from_list(test_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c892b084-076a-4199-8867-f70740b10700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds_sample)\n",
    "print(test_ds_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dda31-c27c-4fae-be89-6c6a52e1a12e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99f9560d-9366-47bb-a419-d9dc7c165ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_and_responses(samples):\n",
    "    output = {\n",
    "        \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "        \"chosen\": samples[\"response_j\"],\n",
    "        \"rejected\": samples[\"response_k\"]\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6827e17d-90d7-4e6b-9311-71303d6c2b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid', 'question', 'date', 'metadata', 'response_j', 'response_k']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_columns = train_ds_sample.column_names\n",
    "original_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "403dc6fc-0d73-4899-945e-ed5bcbf26cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6602aa00ec4f27a4e3d1cee04086bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60d0b79225541dcba932d93e753edc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds_sample_prepared = train_ds_sample.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    # batch_size=1000,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "test_ds_sample_prepared = test_ds_sample.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    # batch_size=1000,\n",
    "    remove_columns=original_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eeb6194c-b5ca-480e-823f-f845ebf3d429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_sample_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2d4ae9a-969a-40ca-9519-eac7590b1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from ml_collections import config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31d4c29d-521a-476f-b56e-13a1536b976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_args = config_dict.ConfigDict()\n",
    "script_args.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "script_args.lora_r = 8\n",
    "script_args.lora_alpha = 16\n",
    "script_args.lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "07ab2c12-8b20-4dac-a606-c52c9c5d1fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "39ef620f-8450-4f8d-a194-99749c30e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = example[\"prompt\"] + example[\"chosen\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b857a94d-c18e-44d6-931b-fc90330eb964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: I have installed the Java 3D API on PC via the exe installer, which simply created a new directory with `j3dcore.jar`, `vecmath.jar`, `j3dutils.jar` in a lib sub-directory and `j3dcore-ogl.dll` in a bin sub-directory.\n",
      "\n",
      "Netbeans had no issues and my code compiled and executed smoothly, however once I built my project and tried to run it from the command prompt I got an `UnsatisfiedLinkError` saying that `no j3dcore-ogl in java.library.path`. \n",
      "\n",
      "Google came to the rescue and gave me 3 viable solutions:\n",
      "\n",
      "* by copying the dll file into my JRE's bin directory\n",
      "* by adding the path of the dll file to the library path (`java -Djava.library.path=dllpath`)\n",
      "* load the dll in the program with `System.load()` (I couldn't get this one to work, actually)\n",
      "\n",
      "My question is: Is there an elegant solution to this problem, that I missed? \n",
      "\n",
      "It seems tedious that for each different PC someone would like to use this program on, he'd have to either copy the dll or add it to the library path before it can run. (Side question: How come Netbeans didn't have a problem with the dll?)\n",
      "\n",
      "Answer: > \n",
      "> Making my Java program easily distributable\n",
      "> \n",
      "> \n",
      "> \n",
      "\n",
      "If you mean 'easy for the end user' look to [Java Web Start](https://stackoverflow.com/tags/java-web-start/info).\n",
      "\n",
      "---\n",
      "\n",
      "A passer-by asks:\n",
      "\n",
      "> \n",
      "> Can you package the dll dependencies with Web Start? \n",
      "> \n",
      "> \n",
      "> \n",
      "\n",
      "Yes, but much, much better. You can package the natives for each platform in separate Jars, and supply them only to the platform that uses that native, even so far as partitioning the download between 32 & 64 bit versions of the natives.\n",
      "\n",
      "JWS puts the natives on the run-time class-path of the application, ready for loading in code.\n",
      "\n",
      "This all happens automatically for the end user, they click a link, approve the trust dialog(s) when asked, and the application installs - possibly with desktop integration, and appears on screen like magic.\n",
      "\n",
      "JWS apps. that use natives need to be distributed as `all-permissions` security level, because the JVM cannot guarantee the actions of anything that 'goes native'.\n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(train_ds_sample_prepared[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ac6096f-75ed-42a4-9269-68cb691b4db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda96f389b584973befe89b51bf13ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = \"right\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=script_args.lora_r,\n",
    "    lora_alpha=script_args.lora_alpha,\n",
    "    lora_dropout=script_args.lora_dropout,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft\",\n",
    "    max_steps=500,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=False,\n",
    "    group_by_length=False,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"sft_llama2\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "    \n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_ds_sample_prepared,\n",
    "    eval_dataset=test_ds_sample_prepared,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe04b9-0ea5-446b-8b04-edfb63159614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fda816-fe3a-4bf8-aae6-3471eaf53591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
